nltk
For Data pre-processing/cleaning as well as tokenization

Re
For regular expressions and further cleaning

ekphrasis
For segmentation and organizing the features,

sklearn
For model training and testing. Had a concise and versatile set of resources to run all the various models we needed.

sklearn.MinMaxScaler
To ensure our dataset were trained fast to allow more testing for further optimization.

tpot/sklearn.GridSearchCV
One of our goals was to further improve our most ormising models, we chose to do hyperparameter tuning using the autoML generative library called tpot as well as sklearnâ€™s GridSearchCV
to make a list of all possible parameters for the brute force method.

matplotlib/sklearn.resample/pandas
Finally for a bit of visualization we used matplotlib and sklearn resample, and pandas for graphing our model results.
